{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increasing-poster",
   "metadata": {},
   "source": [
    "# Notes from Google Pair Programming LLM Course\n",
    "\n",
    "References:\n",
    "\n",
    "* [Deepleaning.AI Course](https://learn.deeplearning.ai/pair-programming-llm/)\n",
    "* [PaLM API](https://developers.generativeai.google/)\n",
    "\n",
    "Introduction with PALM API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import get_api_key\n",
    "import google.generativeai as palm\n",
    "from google.api_core import client_options as client_options_lib\n",
    "\n",
    "palm.configure(\n",
    "    api_key=get_api_key(),\n",
    "    transport=\"rest\",\n",
    "    client_options=client_options_lib.ClientOptions(\n",
    "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
    "    )\n",
    ")\n",
    "# Pick the model that generates text\n",
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model_bison = models[0]\n",
    "model_bison\n",
    "# Helper function to call the PALM API\n",
    "from google.api_core import retry\n",
    "@retry.Retry()\n",
    "def generate_text(prompt, \n",
    "                  model=model_bison, \n",
    "                  temperature=0.0):\n",
    "    return palm.generate_text(prompt=prompt,\n",
    "                              model=model,\n",
    "                              temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-leone",
   "metadata": {},
   "source": [
    "## Priming the Promt\n",
    "\n",
    "#### Prompt template\n",
    "\n",
    "1. priming: getting the LLM ready for the type of task you'll ask it to do.\n",
    "2. question: the specific task.\n",
    "3. decorator: how to provide or format the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "{priming}\n",
    "\n",
    "{question}\n",
    "\n",
    "{decorator}\n",
    "\n",
    "Your solution:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "priming_text = \"You are an expert at writing clear, concise, Python code.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"create a doubly linked list\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-sector",
   "metadata": {},
   "source": [
    "#### Observe how the decorator affects the output\n",
    "- In other non-coding prompt engineering tasks, it's common to use \"chain-of-thought prompting\" by asking the model to work through the task \"step by step\".\n",
    "- For certain tasks like generating code, you may want to experiment with other wording that would make sense if you were asking a developer the same question.\n",
    "\n",
    "In the code cell below, try out option 1 first, then try out option 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "# decorator = \"Work through it step by step, and show your work. One step per line.\"\n",
    "\n",
    "# option 2\n",
    "decorator = \"Insert comments for each line of code.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(priming=priming_text,\n",
    "                                question=question,\n",
    "                                decorator=decorator)\n",
    "# Review \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-computer",
   "metadata": {},
   "source": [
    "Then we can just try different questions such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"create a very large list of random numbers in python, \n",
    "and then write code to sort that list\"\"\"\n",
    "prompt = prompt_template.format(priming=priming_text,\n",
    "                                question=question,\n",
    "                                decorator=decorator)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-chess",
   "metadata": {},
   "source": [
    "Pair Programming scenarios\n",
    "\n",
    "* Improve existing code\n",
    "* Simplify code\n",
    "* Write test cases\n",
    "* Make code more efficient\n",
    "* Debug your code\n",
    "\n",
    "### Scenario 1: Improve existing code\n",
    "- An LLM can help you rewrite your code in the way that's recommended for that particular language.\n",
    "- You can ask an LLM to rewrite your Python code in a way that is more 'Pythonic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "I don't think this code is the best way to do it in Python, can you help me?\n",
    "\n",
    "{question}\n",
    "\n",
    "Please explain, in detail, what you did to improve it.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "def func_x(array)\n",
    "  for i in range(len(array)):\n",
    "    print(array[i])\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-injection",
   "metadata": {},
   "source": [
    "One possible answer could be\n",
    "```python\n",
    "def func_x(array):\n",
    "  print(*array)\n",
    "```\n",
    "\n",
    "I improved the code by using the `*` operator to unpack the array into individual arguments for the `print()` function. This is more concise and efficient than using a `for` loop.\n",
    "\n",
    "Other ways to ask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "I don't think this code is the best way to do it in Python, can you help me?\n",
    "\n",
    "{question}\n",
    "\n",
    "Please explore multiple ways of solving the problem, and explain each.\n",
    "\"\"\"\n",
    "# A more \"Pythonic\"\n",
    "prompt_template = \"\"\"\n",
    "I don't think this code is the best way to do it in Python, can you help me?\n",
    "\n",
    "{question}\n",
    "\n",
    "Please explore multiple ways of solving the problem, \n",
    "and tell me which is the most Pythonic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-seminar",
   "metadata": {},
   "source": [
    "### Scenario 2: Simplify code\n",
    "- Ask the LLM to perform a code review.\n",
    "- Note that adding/removing newline characters may affect the LLM completion that gets output by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "prompt_template = \"\"\"\n",
    "Can you please simplify this code for a linked list in Python?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what you did to modify it, and why.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-memorabilia",
   "metadata": {},
   "source": [
    "After you try option 1, you can modify it to look like option 2 (in this markdown cell) and see how it changes the completion.\n",
    "```Python\n",
    "# option 2\n",
    "prompt_template = \"\"\"\n",
    "Can you please simplify this code for a linked list in Python? \\n\n",
    "You are an expert in Pythonic code.\n",
    "\n",
    "{question}\n",
    "\n",
    "Please comment each line in detail, \\n\n",
    "and explain in detail what you did to modify it, and why.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "class Node:\n",
    "  def __init__(self, dataval=None):\n",
    "    self.dataval = dataval\n",
    "    self.nextval = None\n",
    "\n",
    "class SLinkedList:\n",
    "  def __init__(self):\n",
    "    self.headval = None\n",
    "\n",
    "list1 = SLinkedList()\n",
    "list1.headval = Node(\"Mon\")\n",
    "e2 = Node(\"Tue\")\n",
    "e3 = Node(\"Wed\")\n",
    "list1.headval.nextval = e2\n",
    "e2.nextval = e3\n",
    "\n",
    "\"\"\"\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-wilson",
   "metadata": {},
   "source": [
    "### Scenario 3: Write test cases\n",
    "\n",
    "- It may help to specify that you want the LLM to output \"in code\" to encourage it to write unit tests instead of just returning test cases in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please create test cases in code for this Python code?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what these test cases are designed to achieve.\n",
    "\"\"\"\n",
    "# Note that the code I'm using here was output in the previous\n",
    "# section. Your output code may be different.\n",
    "question = \"\"\"\n",
    "class Node:\n",
    "  def __init__(self, dataval=None):\n",
    "    self.dataval = dataval\n",
    "    self.nextval = None\n",
    "\n",
    "class SLinkedList:\n",
    "  def __init__(self):\n",
    "    self.head = None\n",
    "\n",
    "def create_linked_list(data):\n",
    "  head = Node(data[0])\n",
    "  for i in range(1, len(data)):\n",
    "    node = Node(data[i])\n",
    "    node.nextval = head\n",
    "    head = node\n",
    "  return head\n",
    "\n",
    "list1 = create_linked_list([\"Mon\", \"Tue\", \"Wed\"])\n",
    "\"\"\"\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-earth",
   "metadata": {},
   "source": [
    "### Scenario 4: Make code more efficient\n",
    "- Improve runtime by potentially avoiding inefficient methods (such as ones that use recursion when not needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please make this code more efficient?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what you changed and why.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "# Returns index of x in arr if present, else -1\n",
    "def binary_search(arr, low, high, x):\n",
    "    # Check base case\n",
    "    if high >= low:\n",
    "        mid = (high + low) // 2\n",
    "        if arr[mid] == x:\n",
    "            return mid\n",
    "        elif arr[mid] > x:\n",
    "            return binary_search(arr, low, mid - 1, x)\n",
    "        else:\n",
    "            return binary_search(arr, mid + 1, high, x)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Test array\n",
    "arr = [ 2, 3, 4, 10, 40 ]\n",
    "x = 10\n",
    "\n",
    "# Function call\n",
    "result = binary_search(arr, 0, len(arr)-1, x)\n",
    "\n",
    "if result != -1:\n",
    "    print(\"Element is present at index\", str(result))\n",
    "else:\n",
    "    print(\"Element is not present in array\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-compatibility",
   "metadata": {},
   "source": [
    "### Scenario 5: Debug your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please help me to debug this code?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what you found and why it was a bug.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-impression",
   "metadata": {},
   "source": [
    "# Lesson 4: Technical Debt\n",
    "\n",
    "Complex code being handle down from developer to developer over time, it was writing long time ago, lots of dependencies and hard to change. Here is where LLMs also can help how to understand the code to be able to use it properly.\n",
    "\n",
    "### Ask an LLM to explain a complex code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Complex Code Block\n",
    "# Note: Taken from https://github.com/kevinmgamboa/consciousness/blob/main/train.py\n",
    "CODE_BLOCK = \"\"\"\n",
    "\"\"\"\n",
    "Train a model on the Sleep Dataset\n",
    "Created on Wed Jun  9 21:18:16 2021\n",
    "@author: kevin machado gamboa\n",
    "\"\"\"\n",
    "# -----------------------------------------------------------------------------\n",
    "#                           Libraries Needed\n",
    "# -----------------------------------------------------------------------------\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for training process\n",
    "from sklearn.model_selection import KFold\n",
    "# ML library\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# Personal Libraries\n",
    "from helpers_and_functions import config, main_functions as mpf, utils\n",
    "import modelhub as mh\n",
    "import databases as dbs\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------------------------------\n",
    "#                               Loading sleep dataset\n",
    "# ------------------------------------------------------------------------------------\n",
    "# initialize sleep database\n",
    "sleep = dbs.sleep()\n",
    "# loads [x_epochs, y_labels]\n",
    "sleep.load_epochs_labels(t_files=5, n_test=0.30)\n",
    "# converts labels to [0=>conscious,5* 1=>unconscious]\n",
    "sleep.get_binary_labels()\n",
    "# Normalize the dataset between [-1,1]\n",
    "sleep.transform(mpf.nor_dataset)\n",
    "# applying dataset transformation e.g. 'spectrogram'\n",
    "sleep.transform(mpf.raw_chunks_to_spectrograms, name='spectrogram')\n",
    "# make dataset ready for training\n",
    "sleep.get_ready_for_training()\n",
    "\n",
    "# %%\n",
    "# ------------------------------------------------------------------------------------\n",
    "#                                Cross-Validation\n",
    "# ------------------------------------------------------------------------------------\n",
    "# Creates folder to store experiment\n",
    "date = utils.create_date()\n",
    "os.mkdir('log_savings/sleep_' + date)\n",
    "# confusion matrix per fold variable\n",
    "cm_per_fold = []\n",
    "# number of train epochs\n",
    "train_epochs = 30\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n",
    "#                                               verbose=1, restore_best_weights=True)\n",
    "kfold = KFold(n_splits=config.NUM_FOLDS, shuffle=True)\n",
    "# init fold number\n",
    "fn = 1\n",
    "\n",
    "# stores the models with best acc & other info\n",
    "model_best = {'model': [],\n",
    "          'score': [],\n",
    "          'tra_with': [],\n",
    "          'val_with': [],\n",
    "          'train_history': [],\n",
    "          'initial_weights': [],\n",
    "          'test_acc_per_fold': [],\n",
    "          'test_loss_per_fold': [],\n",
    "          'transformation': sleep.info['transformation']}\n",
    "\n",
    "# training parameters\n",
    "parameters = {'lr': 1e-6,\n",
    "          'num_filters': 10,\n",
    "          'kernel_size': 3,\n",
    "          'dense_units': 10,\n",
    "          'out_size': 1}\n",
    "\n",
    "p_count = 0  # early stopping counter\n",
    "patient = 5  # wait n epochs for error to keep decreasing, is not stop\n",
    "\n",
    "all_folds_best_test_score = 0.0\n",
    "for tra, val in kfold.split(sleep.data['train']['epochs'], sleep.data['train']['labels']):\n",
    "    # Call the hub\n",
    "    hub = mh.simple_cnn_2(param=parameters)\n",
    "    # build model structure\n",
    "    hub.build_model_structure(sleep.info['data_shape'])\n",
    "    # compile model\n",
    "    hub.compile()\n",
    "    # initializing model\n",
    "    model_best_fold = tf.keras.models.clone_model(hub.model)\n",
    "    # initial model weights\n",
    "    ini_wei = hub.model.get_weights()\n",
    "    # initializing training history\n",
    "    train_history = []\n",
    "    # defines an initial score\n",
    "    pre_score = [1.0, 0.0]\n",
    "    # Generate a print\n",
    "    print(100 * '-')\n",
    "    print(f'--------------------------------- Training for fold {fn} ---------------------------------')\n",
    "    # -----------------------------------------------------------------------------\n",
    "    #                               Train-Test Loop\n",
    "    # -----------------------------------------------------------------------------\n",
    "    for n_ep in range(train_epochs):\n",
    "        print('------- train score -------')\n",
    "        # Train the model\n",
    "        train_score = hub.model.pdf(sleep.data['train']['epochs'][tra],\n",
    "                                    sleep.data['train']['labels'][tra],\n",
    "                                    validation_data=(sleep.data['train']['epochs'][val],\n",
    "                                                     sleep.data['train']['labels'][val]),\n",
    "                                    epochs=1)#, callbacks=[early_stop])\n",
    "        print('------- test score -------')\n",
    "        # Evaluates on Test set\n",
    "        test_scores = hub.model.evaluate(sleep.data['test']['epochs'], sleep.data['test']['labels'])\n",
    "        # saving train history\n",
    "        train_score = list(np.concatenate(list(train_score.history.values())))\n",
    "        # Adding test score to train score\n",
    "        train_score.extend(test_scores)\n",
    "        # Train history including the score in test set\n",
    "        train_history.append(train_score)\n",
    "\n",
    "        # Stores the best model in the fold\n",
    "        if test_scores[1] > pre_score[1]:\n",
    "            print(f'new best score in the fold: {test_scores[1]:.4}')\n",
    "            # saves best model INSIDE FOLD\n",
    "            hub.model.save('log_savings/sleep_' + date + '/best_fold_model.h5')\n",
    "            # Saves best model from ALL FOLDS\n",
    "            if test_scores[1] > all_folds_best_test_score:\n",
    "                print(f'new best model from ALL FOLDS {test_scores[1]:.4} ')\n",
    "                all_folds_best_test_score = test_scores[1]\n",
    "                # saves best model\n",
    "                hub.model.save('log_savings/sleep_' + date + '/all_folds_best_model.h5')\n",
    "            # updating previous score\n",
    "            pre_score = copy.copy(test_scores)\n",
    "            # reset the stopping patient counter\n",
    "            p_count = 0\n",
    "        else:  # Stopping criteria:\n",
    "            p_count += 1\n",
    "            if p_count >= patient:\n",
    "                print('Early Stopping !!! Error hasnt decreased')\n",
    "                p_count = 0\n",
    "                break\n",
    "    # -----------------------------------------------------------------------------\n",
    "    #                          Stores Data from Each Fold\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # save train history\n",
    "    model_best['train_history'].append(train_history)\n",
    "    # save best score from fold\n",
    "    train_history = pd.DataFrame(train_history)\n",
    "    idx = train_history[5].idxmax()  # max idx test acc\n",
    "    model_best['score'].append(train_history[5][idx])\n",
    "    # saves segments of data the model was trained with\n",
    "    model_best['tra_with'].append(tra)\n",
    "    model_best['val_with'].append(val)\n",
    "    # save model initial weights\n",
    "    model_best['initial_weights'].append(ini_wei)\n",
    "\n",
    "    print(\n",
    "        f'Best score fold {fn}: {hub.model.metrics_names[0]}: {train_history[4][idx]:.4}; {hub.model.metrics_names[1]}: {train_history[5][idx] * 100:.4}%')\n",
    "    # Adds test score\n",
    "    model_best['test_acc_per_fold'].append(train_history[5][idx] * 100)\n",
    "    model_best['test_loss_per_fold'].append(train_history[4][idx])\n",
    "    # confusion matrix per fold\n",
    "    # -------------------------\n",
    "    # Load best model in fold\n",
    "    model_best_fold = tf.keras.models.load_model('log_savings/sleep_' + date + '/best_fold_model.h5')\n",
    "    # Confusion matrix of best model in fold\n",
    "    cm_per_fold.append(utils.get_confusion_matrix(model_best_fold, sleep.data['test'],\n",
    "                                                  sleep.info['class_balance']['test']['value']))\n",
    "    # Increase fold number\n",
    "    fn += 1\n",
    "# remove model per fold\n",
    "os.remove('log_savings/sleep_' + date + '/best_fold_model.h5')\n",
    "#%%\n",
    "# ------------------------------------------------------------------------------------\n",
    "#                                    Final Results\n",
    "# ------------------------------------------------------------------------------------\n",
    "# confusion matrix dataframe across participants\n",
    "df = utils.cm_fold_to_df(cm_per_fold, model_best['test_loss_per_fold'])\n",
    "utils.boxplot_evaluation_metrics_from_df(df, x_axes='fold')\n",
    "\n",
    "# plots train history for the best model\n",
    "utils.plot_train_test_history(model_best)\n",
    "\n",
    "# Plots the confusion matrix of the best model the folds\n",
    "cm_categories = {0: 'Conscious', 1: 'Unconscious'}\n",
    "labels = [' True Pos', ' False Neg', ' False Pos', ' True Neg']\n",
    "utils.make_confusion_matrix(cm_per_fold[np.argmax(model_best['score'])], group_names=labels, categories=cm_categories,\n",
    "                            class_balance=sleep.info['class_balance']['test']['value'],\n",
    "                            title='Confusion Matrix of Best Model')\n",
    "#%%\n",
    "# ------------------------------------------------------------------------------------\n",
    "#                                       Savings\n",
    "# ------------------------------------------------------------------------------------\n",
    "# import json\n",
    "\n",
    "df.to_csv('log_savings/sleep_' + date + '/folds.csv')\n",
    "np.save('log_savings/sleep_' + date + '/model_best.npy', model_best)\n",
    "# with open('log_savings/sleep_' + date + '/best_model.json', 'wb') as file:\n",
    "#     file.write(json.dumps(model_best).encode(\"utf-8\"))\n",
    "    #json.dump(model_best, file, indent=4)\n",
    "\n",
    "\n",
    "# utils.super_test(tf.keras.models.load_model('log_savings/sleep_' + date + '/all_folds_best_model.h5'),\n",
    "#                  feature_function, dataset='anaesthesia')\n",
    "# #%%\n",
    "# # ------------------------------------------------------------------------------------\n",
    "# #                                    Compares with Benchmark\n",
    "# # ------------------------------------------------------------------------------------\n",
    "# # check if current scores overpasses benchmark\n",
    "# utils.check_benchmark(model_best, database='sleep')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please explain how this code works?\n",
    "\n",
    "{question}\n",
    "\n",
    "Use a lot of detail and make it as clear as possible.\n",
    "\"\"\"\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-interference",
   "metadata": {},
   "source": [
    "### Ask an LLM to document a complex code base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Please write technical documentation for this code and \\n\n",
    "make it easy for a non swift developer to understand:\n",
    "\n",
    "{question}\n",
    "\n",
    "Output the results in markdown\n",
    "\"\"\"\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK)\n",
    ")\n",
    "print(completion.result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
